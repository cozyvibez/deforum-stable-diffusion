{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIManifest/deforum-stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByGXyiHZWM_q"
      },
      "source": [
        "# $ \\color{blue} {\\large \\textsf{Deforum Stable Diffusion v0.7}}$\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). Notebook by [deforum](https://discord.gg/upmXXsrwZc)\n",
        "\n",
        "[Quick Guide](https://docs.google.com/document/d/1RrQv7FntzOuLg4ohjRZPVL7iptIyBhwwbcEYEW2OfcI/edit?usp=sharing) to Deforum v0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "vohUiWo-I2HQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116b7f1a-6831-4159-85d0-33de7c156355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15109 MiB\n",
            "..setting up environment\n",
            "..installing triton and xformers\n",
            "..environment set up in 18 seconds\n"
          ]
        }
      ],
      "source": [
        "#@title $ \\color{blue} {\\large \\textsf{Environment Setup / NVIDIA GPU}}$\n",
        "import subprocess, os, sys\n",
        "sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(f\"{sub_p_res[:-1]}\")\n",
        "import subprocess, time, gc, os, sys\n",
        "\n",
        "def setup_environment():\n",
        "    start_time = time.time()\n",
        "    print_subprocess = False\n",
        "    use_xformers_for_colab = True\n",
        "    try:\n",
        "        ipy = get_ipython()\n",
        "    except:\n",
        "        ipy = 'could not get_ipython'\n",
        "    if 'google.colab' in str(ipy):\n",
        "        print(\"..setting up environment\")\n",
        "\n",
        "        # weird hack\n",
        "        import torch\n",
        "        \n",
        "        all_process = [\n",
        "            ['pip', 'install', 'omegaconf', 'einops==0.4.1', 'pytorch-lightning==1.7.7', 'torchmetrics', 'transformers', 'safetensors', 'kornia'],\n",
        "            ['git', 'clone', 'https://github.com/AIManifest/deforum-stable-diffusion.git'],\n",
        "            ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq','scikit-learn','torchsde','open-clip-torch','numpngw'],\n",
        "        ]\n",
        "        for process in all_process:\n",
        "            running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if print_subprocess:\n",
        "                print(running)\n",
        "        with open('deforum-stable-diffusion/src/k_diffusion/__init__.py', 'w') as f:\n",
        "            f.write('')\n",
        "        sys.path.extend([\n",
        "            'deforum-stable-diffusion/',\n",
        "            'deforum-stable-diffusion/src',\n",
        "        ])\n",
        "        if use_xformers_for_colab:\n",
        "\n",
        "            print(\"..installing triton and xformers\")\n",
        "\n",
        "            all_process = [['pip', 'install', 'triton==2.0.0.dev20221202', 'xformers==0.0.16rc424']]\n",
        "            for process in all_process:\n",
        "                running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "                if print_subprocess:\n",
        "                    print(running)\n",
        "    else:\n",
        "        sys.path.extend([\n",
        "            'src'\n",
        "        ])\n",
        "    end_time = time.time()\n",
        "    print(f\"..environment set up in {end_time-start_time:.0f} seconds\")\n",
        "    return\n",
        "\n",
        "setup_environment()\n",
        "with open('deforum-stable-diffusion/src/k_diffusion/__init__.py', 'w') as f:\n",
        "  f.write('')\n",
        "sys.path.extend([\n",
        "  'deforum-stable-diffusion/',\n",
        "  'deforum-stable-diffusion/src',\n",
        "])\n",
        "import torch\n",
        "import random\n",
        "import clip\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "from IPython import display\n",
        "import glob\n",
        "from types import SimpleNamespace\n",
        "from helpers.save_images import get_output_folder\n",
        "from helpers.settings import load_args\n",
        "from helpers.render import render_animation, render_input_video, render_image_batch, render_interpolation\n",
        "from helpers.model_load import make_linear_decode, load_model, get_model_output_paths\n",
        "from helpers.aesthetics import load_aesthetics_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2LR-dAkoEHd8"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Path Setup}}$\n",
        "\n",
        "def Root():\n",
        "    models_path = \"models\" #@param {type:\"string\"}\n",
        "    configs_path = \"configs\" #@param {type:\"string\"}\n",
        "    output_path = \"outputs\" #@param {type:\"string\"}\n",
        "    mount_google_drive = True #@param {type:\"boolean\"}\n",
        "    models_path_gdrive = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\" #@param {type:\"string\"}\n",
        "    output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown **Model Setup**\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    model_config = \"v1-inference.yaml\" #@param [\"custom\",\"v2-inference.yaml\",\"v2-inference-v.yaml\",\"v1-inference.yaml\"]\n",
        "    model_checkpoint =  \"custom\" #@param [\"custom\",\"v2-1_768-ema-pruned.ckpt\",\"v2-1_512-ema-pruned.ckpt\",\"768-v-ema.ckpt\",\"512-base-ema.ckpt\",\"Protogen_V2.2.ckpt\",\"v1-5-pruned.ckpt\",\"v1-5-pruned-emaonly.ckpt\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\", \"robo-diffusion-v1.ckpt\",\"wd-v1-3-float16.ckpt\"]\n",
        "    custom_config_path = \"/content/drive/MyDrive/AI/models/DreamlikeGen1_0.yaml\" #@param {type:\"string\"}\n",
        "    custom_checkpoint_path = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/DreamNixGen.ckpt\" #@param {type:\"string\"}\n",
        "    embeddings_dir = \"/content/drive/MyDrive/AI/embeddings\" #@param{type:'string'}\n",
        "    return locals()\n",
        "\n",
        "root = Root()\n",
        "root = SimpleNamespace(**root)\n",
        "\n",
        "root.models_path, root.output_path = get_model_output_paths(root)\n",
        "root.model, root.device = load_model(root, load_on_run_all=True, check_sha256=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CZYWb5prR-8Q"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Load Textual Inversion!}}$\n",
        "model = root.model\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "import html\n",
        "import datetime\n",
        "import csv\n",
        "import safetensors.torch\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, PngImagePlugin\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "TextualInversionTemplate = namedtuple(\"TextualInversionTemplate\", [\"name\", \"path\"])\n",
        "textual_inversion_templates = {}\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, vec, name, step=None):\n",
        "        self.vec = vec\n",
        "        self.name = name\n",
        "        self.step = step\n",
        "        self.shape = None\n",
        "        self.vectors = 0\n",
        "        self.cached_checksum = None\n",
        "        self.sd_checkpoint = None\n",
        "        self.sd_checkpoint_name = None\n",
        "        self.optimizer_state_dict = None\n",
        "        self.filename = None\n",
        "\n",
        "    def checksum(self):\n",
        "        if self.cached_checksum is not None:\n",
        "            return self.cached_checksum\n",
        "\n",
        "        def const_hash(a):\n",
        "            r = 0\n",
        "            for v in a:\n",
        "                r = (r * 281 ^ int(v) * 997) & 0xFFFFFFFF\n",
        "            return r\n",
        "\n",
        "        self.cached_checksum = f'{const_hash(self.vec.reshape(-1) * 100) & 0xffff:04x}'\n",
        "        return self.cached_checksum\n",
        "\n",
        "\n",
        "class DirWithTextualInversionEmbeddings:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.mtime = None\n",
        "\n",
        "    def has_changed(self):\n",
        "        if not os.path.isdir(self.path):\n",
        "            return False\n",
        "\n",
        "        mt = os.path.getmtime(self.path)\n",
        "        if self.mtime is None or mt > self.mtime:\n",
        "            return True\n",
        "\n",
        "    def update(self):\n",
        "        if not os.path.isdir(self.path):\n",
        "            return\n",
        "\n",
        "        self.mtime = os.path.getmtime(self.path)\n",
        "\n",
        "\n",
        "class EmbeddingDatabase:\n",
        "    def __init__(self):\n",
        "        self.ids_lookup = {}\n",
        "        self.word_embeddings = {}\n",
        "        self.skipped_embeddings = {}\n",
        "        self.expected_shape = -1\n",
        "        self.embedding_dirs = {}\n",
        "\n",
        "    def add_embedding_dir(self, path):\n",
        "        self.embedding_dirs[path] = DirWithTextualInversionEmbeddings(path)\n",
        "\n",
        "    def clear_embedding_dirs(self):\n",
        "        self.embedding_dirs.clear()\n",
        "\n",
        "    def register_embedding(self, embedding, model):\n",
        "        self.word_embeddings[embedding.name] = embedding\n",
        "\n",
        "        ids = model.cond_stage_model.tokenize([embedding.name])[0]\n",
        "\n",
        "        first_id = ids[0]\n",
        "        if first_id not in self.ids_lookup:\n",
        "            self.ids_lookup[first_id] = []\n",
        "\n",
        "        self.ids_lookup[first_id] = sorted(self.ids_lookup[first_id] + [(ids, embedding)], key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def get_expected_shape(self):\n",
        "        vec = root.model.cond_stage_model.encode_embedding_init_text(\",\", 1)\n",
        "        return vec.shape[1]\n",
        "\n",
        "    def load_from_file(self, path, filename):\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        ext = ext.upper()\n",
        "\n",
        "        if ext in ['.PNG', '.WEBP', '.JXL', '.AVIF']:\n",
        "            _, second_ext = os.path.splitext(name)\n",
        "            if second_ext.upper() == '.PREVIEW':\n",
        "                return\n",
        "\n",
        "            embed_image = Image.open(path)\n",
        "            if hasattr(embed_image, 'text') and 'sd-ti-embedding' in embed_image.text:\n",
        "                data = embedding_from_b64(embed_image.text['sd-ti-embedding'])\n",
        "                name = data.get('name', name)\n",
        "            else:\n",
        "                data = extract_image_data_embed(embed_image)\n",
        "                name = data.get('name', name)\n",
        "        elif ext in ['.BIN', '.PT']:\n",
        "            data = torch.load(path, map_location=\"cpu\")\n",
        "        elif ext in ['.SAFETENSORS']:\n",
        "            data = safetensors.torch.load_file(path, device=\"cpu\")\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # textual inversion embeddings\n",
        "        if 'string_to_param' in data:\n",
        "            param_dict = data['string_to_param']\n",
        "            if hasattr(param_dict, '_parameters'):\n",
        "                param_dict = getattr(param_dict, '_parameters')  # fix for torch 1.12.1 loading saved file from torch 1.11\n",
        "            assert len(param_dict) == 1, 'embedding file has multiple terms in it'\n",
        "            emb = next(iter(param_dict.items()))[1]\n",
        "        # diffuser concepts\n",
        "        elif type(data) == dict and type(next(iter(data.values()))) == torch.Tensor:\n",
        "            assert len(data.keys()) == 1, 'embedding file has multiple terms in it'\n",
        "\n",
        "            emb = next(iter(data.values()))\n",
        "            if len(emb.shape) == 1:\n",
        "                emb = emb.unsqueeze(0)\n",
        "        else:\n",
        "            raise Exception(f\"Couldn't identify {filename} as neither textual inversion embedding nor diffuser concept.\")\n",
        "\n",
        "        vec = emb.detach().to(root.device, dtype=torch.float32)\n",
        "        embedding = Embedding(vec, name)\n",
        "        embedding.step = data.get('step', None)\n",
        "        embedding.sd_checkpoint = data.get('sd_checkpoint', None)\n",
        "        embedding.sd_checkpoint_name = data.get('sd_checkpoint_name', None)\n",
        "        embedding.vectors = vec.shape[0]\n",
        "        embedding.shape = vec.shape[-1]\n",
        "        embedding.filename = path\n",
        "\n",
        "        if self.expected_shape == -1 or self.expected_shape == embedding.shape:\n",
        "            self.register_embedding(embedding, root.model)\n",
        "        else:\n",
        "            self.skipped_embeddings[name] = embedding\n",
        "\n",
        "    def load_from_dir(self, embdir):\n",
        "        if not os.path.isdir(embdir.path):\n",
        "            return\n",
        "\n",
        "        for root, dirs, fns in os.walk(embdir.path):\n",
        "            for fn in fns:\n",
        "                try:\n",
        "                    fullfn = os.path.join(root, fn)\n",
        "\n",
        "                    if os.stat(fullfn).st_size == 0:\n",
        "                        continue\n",
        "\n",
        "                    self.load_from_file(fullfn, fn)\n",
        "                except Exception:\n",
        "                    print(f\"Error loading embedding {fn}:\", file=sys.stderr)\n",
        "                    print(traceback.format_exc(), file=sys.stderr)\n",
        "                    continue\n",
        "\n",
        "    def load_textual_inversion_embeddings(self, force_reload=False):\n",
        "        if not force_reload:\n",
        "            need_reload = False\n",
        "            for path, embdir in self.embedding_dirs.items():\n",
        "                if embdir.has_changed():\n",
        "                    need_reload = True\n",
        "                    break\n",
        "\n",
        "            if not need_reload:\n",
        "                return\n",
        "\n",
        "        self.ids_lookup.clear()\n",
        "        self.word_embeddings.clear()\n",
        "        self.skipped_embeddings.clear()\n",
        "        self.expected_shape = self.get_expected_shape()\n",
        "\n",
        "        for path, embdir in self.embedding_dirs.items():\n",
        "            self.load_from_dir(embdir)\n",
        "            embdir.update()\n",
        "\n",
        "        print(f\"Textual inversion embeddings loaded({len(self.word_embeddings)}): {', '.join(self.word_embeddings.keys())}\")\n",
        "        if len(self.skipped_embeddings) > 0:\n",
        "            print(f\"Textual inversion embeddings skipped({len(self.skipped_embeddings)}): {', '.join(self.skipped_embeddings.keys())}\")\n",
        "\n",
        "    def find_embedding_at_position(self, tokens, offset):\n",
        "        token = tokens[offset]\n",
        "        possible_matches = self.ids_lookup.get(token, None)\n",
        "\n",
        "        if possible_matches is None:\n",
        "            return None, None\n",
        "\n",
        "        for ids, embedding in possible_matches:\n",
        "            if tokens[offset:offset + len(ids)] == ids:\n",
        "                return embedding, len(ids)\n",
        "\n",
        "        return None, None\n",
        "#@title sd_hijack\n",
        "import torch\n",
        "from torch.nn.functional import silu\n",
        "# import drive.MyDrive.textual_inversion\n",
        "import ldm.modules.attention\n",
        "import ldm.modules.diffusionmodules.model\n",
        "import ldm.modules.diffusionmodules.openaimodel\n",
        "import ldm.models.diffusion.ddim\n",
        "import ldm.models.diffusion.plms\n",
        "import ldm.modules.encoders.modules\n",
        "\n",
        "attention_CrossAttention_forward = ldm.modules.attention.CrossAttention.forward\n",
        "diffusionmodules_model_nonlinearity = ldm.modules.diffusionmodules.model.nonlinearity\n",
        "diffusionmodules_model_AttnBlock_forward = ldm.modules.diffusionmodules.model.AttnBlock.forward\n",
        "\n",
        "# new memory efficient cross attention blocks do not support hypernets and we already\n",
        "# have memory efficient cross attention anyway, so this disables SD2.0's memory efficient cross attention\n",
        "ldm.modules.attention.MemoryEfficientCrossAttention = ldm.modules.attention.CrossAttention\n",
        "ldm.modules.attention.BasicTransformerBlock.ATTENTION_MODES[\"softmax-xformers\"] = ldm.modules.attention.CrossAttention\n",
        "\n",
        "# silence new console spam from SD2\n",
        "ldm.modules.attention.print = lambda *args: None\n",
        "ldm.modules.diffusionmodules.model.print = lambda *args: None\n",
        "\n",
        "def fix_checkpoint():\n",
        "    \"\"\"checkpoints are now added and removed in embedding/hypernet code, since torch doesn't want\n",
        "    checkpoints to be added when not training (there's a warning)\"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class StableDiffusionModelHijack:\n",
        "    fixes = None\n",
        "    comments = []\n",
        "    layers = None\n",
        "    circular_enabled = False\n",
        "    clip = None\n",
        "    optimization_method = None\n",
        "\n",
        "    embedding_db = EmbeddingDatabase()\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embedding_db.add_embedding_dir(root.embeddings_dir)\n",
        "\n",
        "    def hijack(self, m):\n",
        "\n",
        "        if type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenCLIPEmbedder:\n",
        "            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n",
        "            model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.token_embedding, self)\n",
        "            m.cond_stage_model = FrozenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n",
        "\n",
        "        elif type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder:\n",
        "            m.cond_stage_model.model.token_embedding = EmbeddingsWithFixes(m.cond_stage_model.model.token_embedding, self)\n",
        "            m.cond_stage_model = FrozenOpenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n",
        "\n",
        "        # self.optimization_method = apply_optimizations()\n",
        "\n",
        "        self.clip = m.cond_stage_model\n",
        "\n",
        "        def flatten(el):\n",
        "            flattened = [flatten(children) for children in el.children()]\n",
        "            res = [el]\n",
        "            for c in flattened:\n",
        "                res += c\n",
        "            return res\n",
        "\n",
        "        self.layers = flatten(m)\n",
        "\n",
        "    def undo_hijack(self, m):\n",
        "        \n",
        "        if type(m.cond_stage_model) == FrozenCLIPEmbedderWithCustomWords:\n",
        "            m.cond_stage_model = m.cond_stage_model.wrapped\n",
        "\n",
        "            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n",
        "            if type(model_embeddings.token_embedding) == EmbeddingsWithFixes:\n",
        "                model_embeddings.token_embedding = model_embeddings.token_embedding.wrapped\n",
        "        elif type(m.cond_stage_model) == FrozenOpenCLIPEmbedderWithCustomWords:\n",
        "            m.cond_stage_model.wrapped.model.token_embedding = m.cond_stage_model.wrapped.model.token_embedding.wrapped\n",
        "            m.cond_stage_model = m.cond_stage_model.wrapped\n",
        "\n",
        "        self.apply_circular(False)\n",
        "        self.layers = None\n",
        "        self.clip = None\n",
        "\n",
        "    def apply_circular(self, enable):\n",
        "        if self.circular_enabled == enable:\n",
        "            return\n",
        "\n",
        "        self.circular_enabled = enable\n",
        "\n",
        "        for layer in [layer for layer in self.layers if type(layer) == torch.nn.Conv2d]:\n",
        "            layer.padding_mode = 'circular' if enable else 'zeros'\n",
        "\n",
        "    def clear_comments(self):\n",
        "        self.comments = []\n",
        "\n",
        "    def get_prompt_lengths(self, text):\n",
        "        _, token_count = self.clip.process_texts([text])\n",
        "\n",
        "        return token_count, self.clip.get_target_prompt_token_count(token_count)\n",
        "\n",
        "\n",
        "class EmbeddingsWithFixes(torch.nn.Module):\n",
        "    def __init__(self, wrapped, embeddings):\n",
        "        super().__init__()\n",
        "        self.wrapped = wrapped\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_fixes = self.embeddings.fixes\n",
        "        self.embeddings.fixes = None\n",
        "\n",
        "        inputs_embeds = self.wrapped(input_ids)\n",
        "\n",
        "        if batch_fixes is None or len(batch_fixes) == 0 or max([len(x) for x in batch_fixes]) == 0:\n",
        "            return inputs_embeds\n",
        "\n",
        "        vecs = []\n",
        "        for fixes, tensor in zip(batch_fixes, inputs_embeds):\n",
        "            for offset, embedding in fixes:\n",
        "                emb = embedding.vec\n",
        "                emb_len = min(tensor.shape[0] - offset - 1, emb.shape[0])\n",
        "                tensor = torch.cat([tensor[0:offset + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])\n",
        "\n",
        "            vecs.append(tensor)\n",
        "\n",
        "        return torch.stack(vecs)\n",
        "\n",
        "\n",
        "def add_circular_option_to_conv_2d():\n",
        "    conv2d_constructor = torch.nn.Conv2d.__init__\n",
        "\n",
        "    def conv2d_constructor_circular(self, *args, **kwargs):\n",
        "        return conv2d_constructor(self, *args, padding_mode='circular', **kwargs)\n",
        "\n",
        "    torch.nn.Conv2d.__init__ = conv2d_constructor_circular\n",
        "\n",
        "\n",
        "model_hijack = StableDiffusionModelHijack()\n",
        "\n",
        "\n",
        "def register_buffer(self, name, attr):\n",
        "    \"\"\"\n",
        "    Fix register buffer bug for Mac OS.\n",
        "    \"\"\"\n",
        "\n",
        "    if type(attr) == torch.Tensor:\n",
        "        if attr.device != root.device:\n",
        "            attr = attr.to(device=root.device, dtype=(torch.float32 if devices.device.type == 'mps' else None))\n",
        "\n",
        "    setattr(self, name, attr)\n",
        "\n",
        "\n",
        "ldm.models.diffusion.ddim.DDIMSampler.register_buffer = register_buffer\n",
        "ldm.models.diffusion.plms.PLMSSampler.register_buffer = register_buffer\n",
        "\n",
        "#@title hijack_checkpoint\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "import ldm.modules.attention\n",
        "import ldm.modules.diffusionmodules.openaimodel\n",
        "\n",
        "\n",
        "def BasicTransformerBlock_forward(self, x, context=None):\n",
        "    return checkpoint(self._forward, x, context)\n",
        "\n",
        "\n",
        "def AttentionBlock_forward(self, x):\n",
        "    return checkpoint(self._forward, x)\n",
        "\n",
        "\n",
        "def ResBlock_forward(self, x, emb):\n",
        "    return checkpoint(self._forward, x, emb)\n",
        "\n",
        "\n",
        "stored = []\n",
        "\n",
        "\n",
        "def add():\n",
        "    if len(stored) != 0:\n",
        "        return\n",
        "\n",
        "    stored.extend([\n",
        "        ldm.modules.attention.BasicTransformerBlock.forward,\n",
        "        ldm.modules.diffusionmodules.openaimodel.ResBlock.forward,\n",
        "        ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward\n",
        "    ])\n",
        "\n",
        "    ldm.modules.attention.BasicTransformerBlock.forward = BasicTransformerBlock_forward\n",
        "    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = ResBlock_forward\n",
        "    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = AttentionBlock_forward\n",
        "\n",
        "\n",
        "def remove():\n",
        "    if len(stored) == 0:\n",
        "        return\n",
        "\n",
        "    ldm.modules.attention.BasicTransformerBlock.forward = stored[0]\n",
        "    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = stored[1]\n",
        "    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = stored[2]\n",
        "\n",
        "    stored.clear()\n",
        "\n",
        "\n",
        "#@title hijack_clip\n",
        "\n",
        "import math\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "\n",
        "class PromptChunk:\n",
        "    \"\"\"\n",
        "    This object contains token ids, weight (multipliers:1.4) and textual inversion embedding info for a chunk of prompt.\n",
        "    If a prompt is short, it is represented by one PromptChunk, otherwise, multiple are necessary.\n",
        "    Each PromptChunk contains an exact amount of tokens - 77, which includes one for start and end token,\n",
        "    so just 75 tokens from prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokens = []\n",
        "        self.multipliers = []\n",
        "        self.fixes = []\n",
        "\n",
        "\n",
        "PromptChunkFix = namedtuple('PromptChunkFix', ['offset', 'embedding'])\n",
        "\"\"\"An object of this type is a marker showing that textual inversion embedding's vectors have to placed at offset in the prompt\n",
        "chunk. Thos objects are found in PromptChunk.fixes and, are placed into FrozenCLIPEmbedderWithCustomWordsBase.hijack.fixes, and finally\n",
        "are applied by sd_hijack.EmbeddingsWithFixes's forward function.\"\"\"\n",
        "\n",
        "\n",
        "class FrozenCLIPEmbedderWithCustomWordsBase(torch.nn.Module):\n",
        "    \"\"\"A pytorch module that is a wrapper for FrozenCLIPEmbedder module. it enhances FrozenCLIPEmbedder, making it possible to\n",
        "    have unlimited prompt length and assign weights to tokens in prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__()\n",
        "\n",
        "        self.wrapped = wrapped\n",
        "        \"\"\"Original FrozenCLIPEmbedder module; can also be FrozenOpenCLIPEmbedder or xlmr.BertSeriesModelWithTransformation,\n",
        "        depending on model.\"\"\"\n",
        "\n",
        "        self.hijack: StableDiffusionModelHijack = hijack\n",
        "        self.chunk_length = 75\n",
        "\n",
        "    def empty_chunk(self):\n",
        "        \"\"\"creates an empty PromptChunk and returns it\"\"\"\n",
        "\n",
        "        chunk = PromptChunk()\n",
        "        chunk.tokens = [self.id_start] + [self.id_end] * (self.chunk_length + 1)\n",
        "        chunk.multipliers = [1.0] * (self.chunk_length + 2)\n",
        "        return chunk\n",
        "\n",
        "    def get_target_prompt_token_count(self, token_count):\n",
        "        \"\"\"returns the maximum number of tokens a prompt of a known length can have before it requires one more PromptChunk to be represented\"\"\"\n",
        "\n",
        "        return math.ceil(max(token_count, 1) / self.chunk_length) * self.chunk_length\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        \"\"\"Converts a batch of texts into a batch of token ids\"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        \"\"\"\n",
        "        converts a batch of token ids (in python lists) into a single tensor with numeric respresentation of those tokens;\n",
        "        All python lists with tokens are assumed to have same length, usually 77.\n",
        "        if input is a list with B elements and each element has T tokens, expected output shape is (B, T, C), where C depends on\n",
        "        model - can be 768 and 1024.\n",
        "        Among other things, this call will read self.hijack.fixes, apply it to its inputs, and clear it (setting it to None).\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        \"\"\"Converts text into a tensor with this text's tokens' embeddings. Note that those are embeddings before they are passed through\n",
        "        transformers. nvpt is used as a maximum length in tokens. If text produces less teokens than nvpt, only this many is returned.\"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def tokenize_line(self, line):\n",
        "        \"\"\"\n",
        "        this transforms a single prompt into a list of PromptChunk objects - as many as needed to\n",
        "        represent the prompt.\n",
        "        Returns the list and the total number of tokens in the prompt.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        parsed = [[line, 1.0]]\n",
        "\n",
        "        tokenized = self.tokenize([text for text, _ in parsed])\n",
        "\n",
        "        chunks = []\n",
        "        chunk = PromptChunk()\n",
        "        token_count = 0\n",
        "        last_comma = -1\n",
        "\n",
        "        def next_chunk(is_last=False):\n",
        "            \"\"\"puts current chunk into the list of results and produces the next one - empty;\n",
        "            if is_last is true, tokens <end-of-text> tokens at the end won't add to token_count\"\"\"\n",
        "            nonlocal token_count\n",
        "            nonlocal last_comma\n",
        "            nonlocal chunk\n",
        "\n",
        "            if is_last:\n",
        "                token_count += len(chunk.tokens)\n",
        "            else:\n",
        "                token_count += self.chunk_length\n",
        "\n",
        "            to_add = self.chunk_length - len(chunk.tokens)\n",
        "            if to_add > 0:\n",
        "                chunk.tokens += [self.id_end] * to_add\n",
        "                chunk.multipliers += [1.0] * to_add\n",
        "\n",
        "            chunk.tokens = [self.id_start] + chunk.tokens + [self.id_end]\n",
        "            chunk.multipliers = [1.0] + chunk.multipliers + [1.0]\n",
        "\n",
        "            last_comma = -1\n",
        "            chunks.append(chunk)\n",
        "            chunk = PromptChunk()\n",
        "\n",
        "        for tokens, (text, weight) in zip(tokenized, parsed):\n",
        "            if text == 'BREAK' and weight == -1:\n",
        "                next_chunk()\n",
        "                continue\n",
        "\n",
        "            position = 0\n",
        "            while position < len(tokens):\n",
        "                token = tokens[position]\n",
        "\n",
        "                if token == self.comma_token:\n",
        "                    last_comma = len(chunk.tokens)\n",
        "\n",
        "                # this is when we are at the end of alloted 75 tokens for the current chunk, and the current token is not a comma. opts.comma_padding_backtrack\n",
        "                # is a setting that specifies that if there is a comma nearby, the text after the comma should be moved out of this chunk and into the next.\n",
        "\n",
        "                if len(chunk.tokens) == self.chunk_length:\n",
        "                    next_chunk()\n",
        "\n",
        "                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, position)\n",
        "                if embedding is None:\n",
        "                    chunk.tokens.append(token)\n",
        "                    chunk.multipliers.append(weight)\n",
        "                    position += 1\n",
        "                    continue\n",
        "\n",
        "                emb_len = int(embedding.vec.shape[0])\n",
        "                if len(chunk.tokens) + emb_len > self.chunk_length:\n",
        "                    next_chunk()\n",
        "\n",
        "                chunk.fixes.append(PromptChunkFix(len(chunk.tokens), embedding))\n",
        "\n",
        "                chunk.tokens += [0] * emb_len\n",
        "                chunk.multipliers += [weight] * emb_len\n",
        "                position += embedding_length_in_tokens\n",
        "\n",
        "        if len(chunk.tokens) > 0 or len(chunks) == 0:\n",
        "            next_chunk(is_last=True)\n",
        "\n",
        "        return chunks, token_count\n",
        "\n",
        "    def process_texts(self, texts):\n",
        "        \"\"\"\n",
        "        Accepts a list of texts and calls tokenize_line() on each, with cache. Returns the list of results and maximum\n",
        "        length, in tokens, of all texts.\n",
        "        \"\"\"\n",
        "\n",
        "        token_count = 0\n",
        "\n",
        "        cache = {}\n",
        "        batch_chunks = []\n",
        "        for line in texts:\n",
        "            if line in cache:\n",
        "                chunks = cache[line]\n",
        "            else:\n",
        "                chunks, current_token_count = self.tokenize_line(line)\n",
        "                token_count = max(current_token_count, token_count)\n",
        "\n",
        "                cache[line] = chunks\n",
        "\n",
        "            batch_chunks.append(chunks)\n",
        "\n",
        "        return batch_chunks, token_count\n",
        "\n",
        "    def forward(self, texts):\n",
        "        \"\"\"\n",
        "        Accepts an array of texts; Passes texts through transformers network to create a tensor with numerical representation of those texts.\n",
        "        Returns a tensor with shape of (B, T, C), where B is length of the array; T is length, in tokens, of texts (including padding) - T will\n",
        "        be a multiple of 77; and C is dimensionality of each token - for SD1 it's 768, and for SD2 it's 1024.\n",
        "        An example shape returned by this function can be: (2, 77, 768).\n",
        "        Webui usually sends just one text at a time through this function - the only time when texts is an array with more than one elemenet\n",
        "        is when you do prompt editing: \"a picture of a [cat:dog:0.4] eating ice cream\"\n",
        "        \"\"\"\n",
        "\n",
        "        # if opts.use_old_emphasis_implementation:\n",
        "        #     import modules.sd_hijack_clip_old\n",
        "        #     return modules.sd_hijack_clip_old.forward_old(self, texts)\n",
        "\n",
        "        batch_chunks, token_count = self.process_texts(texts)\n",
        "\n",
        "        used_embeddings = {}\n",
        "        chunk_count = max([len(x) for x in batch_chunks])\n",
        "\n",
        "        zs = []\n",
        "        for i in range(chunk_count):\n",
        "            batch_chunk = [chunks[i] if i < len(chunks) else self.empty_chunk() for chunks in batch_chunks]\n",
        "\n",
        "            tokens = [x.tokens for x in batch_chunk]\n",
        "            multipliers = [x.multipliers for x in batch_chunk]\n",
        "            self.hijack.fixes = [x.fixes for x in batch_chunk]\n",
        "\n",
        "            for fixes in self.hijack.fixes:\n",
        "                for position, embedding in fixes:\n",
        "                    used_embeddings[embedding.name] = embedding\n",
        "\n",
        "            z = self.process_tokens(tokens, multipliers)\n",
        "            zs.append(z)\n",
        "\n",
        "        if len(used_embeddings) > 0:\n",
        "            embeddings_list = \", \".join([f'{name} [{embedding.checksum()}]' for name, embedding in used_embeddings.items()])\n",
        "            self.hijack.comments.append(f\"Used embeddings: {embeddings_list}\")\n",
        "\n",
        "        return torch.hstack(zs)\n",
        "\n",
        "    def process_tokens(self, remade_batch_tokens, batch_multipliers):\n",
        "        \"\"\"\n",
        "        sends one single prompt chunk to be encoded by transformers neural network.\n",
        "        remade_batch_tokens is a batch of tokens - a list, where every element is a list of tokens; usually\n",
        "        there are exactly 77 tokens in the list. batch_multipliers is the same but for multipliers instead of tokens.\n",
        "        Multipliers are used to give more or less weight to the outputs of transformers network. Each multiplier\n",
        "        corresponds to one token.\n",
        "        \"\"\"\n",
        "        tokens = torch.asarray(remade_batch_tokens).to(root.device)\n",
        "\n",
        "        # this is for SD2: SD1 uses the same token for padding and end of text, while SD2 uses different ones.\n",
        "        if self.id_end != self.id_pad:\n",
        "            for batch_pos in range(len(remade_batch_tokens)):\n",
        "                index = remade_batch_tokens[batch_pos].index(self.id_end)\n",
        "                tokens[batch_pos, index+1:tokens.shape[1]] = self.id_pad\n",
        "\n",
        "        z = self.encode_with_transformers(tokens)\n",
        "\n",
        "        # restoring original mean is likely not correct, but it seems to work well to prevent artifacts that happen otherwise\n",
        "        batch_multipliers = torch.asarray(batch_multipliers).to(root.device)\n",
        "        original_mean = z.mean()\n",
        "        z = z * batch_multipliers.reshape(batch_multipliers.shape + (1,)).expand(z.shape)\n",
        "        new_mean = z.mean()\n",
        "        z = z * (original_mean / new_mean)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "class FrozenCLIPEmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWordsBase):\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__(wrapped, hijack)\n",
        "        self.tokenizer = wrapped.tokenizer\n",
        "\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "\n",
        "        self.comma_token = vocab.get(',</w>', None)\n",
        "\n",
        "        self.token_mults = {}\n",
        "        tokens_with_parens = [(k, v) for k, v in vocab.items() if '(' in k or ')' in k or '[' in k or ']' in k]\n",
        "        for text, ident in tokens_with_parens:\n",
        "            mult = 1.0\n",
        "            for c in text:\n",
        "                if c == '[':\n",
        "                    mult /= 1.1\n",
        "                if c == ']':\n",
        "                    mult *= 1.1\n",
        "                if c == '(':\n",
        "                    mult *= 1.1\n",
        "                if c == ')':\n",
        "                    mult /= 1.1\n",
        "\n",
        "            if mult != 1.0:\n",
        "                self.token_mults[ident] = mult\n",
        "\n",
        "        self.id_start = self.wrapped.tokenizer.bos_token_id\n",
        "        self.id_end = self.wrapped.tokenizer.eos_token_id\n",
        "        self.id_pad = self.id_end\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        tokenized = self.wrapped.tokenizer(texts, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        CLIP_stop_at_last_layers=1\n",
        "        outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=CLIP_stop_at_last_layers)\n",
        "\n",
        "        z = outputs.last_hidden_state\n",
        "\n",
        "        return z\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        embedding_layer = self.wrapped.transformer.text_model.embeddings\n",
        "        ids = self.wrapped.tokenizer(init_text, max_length=nvpt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "        embedded = embedding_layer.token_embedding.wrapped(ids.to(embedding_layer.token_embedding.wrapped.weight.device)).squeeze(0)\n",
        "\n",
        "        return embedded\n",
        "\n",
        "#@title hijack_clip_old\n",
        "# from drive.MyDrive import sd_hijack_clip\n",
        "def process_text_old(self: FrozenCLIPEmbedderWithCustomWordsBase, texts):\n",
        "    id_start = self.id_start\n",
        "    id_end = self.id_end\n",
        "    maxlen = self.wrapped.max_length  # you get to stay at 77\n",
        "    used_custom_terms = []\n",
        "    remade_batch_tokens = []\n",
        "    hijack_comments = []\n",
        "    hijack_fixes = []\n",
        "    token_count = 0\n",
        "\n",
        "    cache = {}\n",
        "    batch_tokens = self.tokenize(texts)\n",
        "    batch_multipliers = []\n",
        "    for tokens in batch_tokens:\n",
        "        tuple_tokens = tuple(tokens)\n",
        "\n",
        "        if tuple_tokens in cache:\n",
        "            remade_tokens, fixes, multipliers = cache[tuple_tokens]\n",
        "        else:\n",
        "            fixes = []\n",
        "            remade_tokens = []\n",
        "            multipliers = []\n",
        "            mult = 1.0\n",
        "\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                token = tokens[i]\n",
        "\n",
        "                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, i)\n",
        "\n",
        "                mult_change = self.token_mults.get(token)\n",
        "                if mult_change is not None:\n",
        "                    mult *= mult_change\n",
        "                    i += 1\n",
        "                elif embedding is None:\n",
        "                    remade_tokens.append(token)\n",
        "                    multipliers.append(mult)\n",
        "                    i += 1\n",
        "                else:\n",
        "                    emb_len = int(embedding.vec.shape[0])\n",
        "                    fixes.append((len(remade_tokens), embedding))\n",
        "                    remade_tokens += [0] * emb_len\n",
        "                    multipliers += [mult] * emb_len\n",
        "                    used_custom_terms.append((embedding.name, embedding.checksum()))\n",
        "                    i += embedding_length_in_tokens\n",
        "\n",
        "            if len(remade_tokens) > maxlen - 2:\n",
        "                vocab = {v: k for k, v in self.wrapped.tokenizer.get_vocab().items()}\n",
        "                ovf = remade_tokens[maxlen - 2:]\n",
        "                overflowing_words = [vocab.get(int(x), \"\") for x in ovf]\n",
        "                overflowing_text = self.wrapped.tokenizer.convert_tokens_to_string(''.join(overflowing_words))\n",
        "                hijack_comments.append(f\"Warning: too many input tokens; some ({len(overflowing_words)}) have been truncated:\\n{overflowing_text}\\n\")\n",
        "\n",
        "            token_count = len(remade_tokens)\n",
        "            remade_tokens = remade_tokens + [id_end] * (maxlen - 2 - len(remade_tokens))\n",
        "            remade_tokens = [id_start] + remade_tokens[0:maxlen - 2] + [id_end]\n",
        "            cache[tuple_tokens] = (remade_tokens, fixes, multipliers)\n",
        "\n",
        "        multipliers = multipliers + [1.0] * (maxlen - 2 - len(multipliers))\n",
        "        multipliers = [1.0] + multipliers[0:maxlen - 2] + [1.0]\n",
        "\n",
        "        remade_batch_tokens.append(remade_tokens)\n",
        "        hijack_fixes.append(fixes)\n",
        "        batch_multipliers.append(multipliers)\n",
        "    return batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count\n",
        "\n",
        "\n",
        "def forward_old(self: FrozenCLIPEmbedderWithCustomWordsBase, texts):\n",
        "    batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count = process_text_old(self, texts)\n",
        "\n",
        "    self.hijack.comments += hijack_comments\n",
        "\n",
        "    if len(used_custom_terms) > 0:\n",
        "        self.hijack.comments.append(\"Used embeddings: \" + \", \".join([f'{word} [{checksum}]' for word, checksum in used_custom_terms]))\n",
        "\n",
        "    self.hijack.fixes = hijack_fixes\n",
        "    return self.process_tokens(remade_batch_tokens, batch_multipliers)\n",
        "\n",
        "#@title hijack_open_clip\n",
        "\n",
        "import open_clip.tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = open_clip.tokenizer._tokenizer\n",
        "\n",
        "\n",
        "class FrozenOpenCLIPEmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWordsBase):\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__(wrapped, hijack)\n",
        "\n",
        "        self.comma_token = [v for k, v in tokenizer.encoder.items() if k == ',</w>'][0]\n",
        "        self.id_start = tokenizer.encoder[\"<start_of_text>\"]\n",
        "        self.id_end = tokenizer.encoder[\"<end_of_text>\"]\n",
        "        self.id_pad = 0\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "\n",
        "        tokenized = [tokenizer.encode(text) for text in texts]\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        # set self.wrapped.layer_idx here according to opts.CLIP_stop_at_last_layers\n",
        "        z = self.wrapped.encode_with_transformer(tokens)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        ids = tokenizer.encode(init_text)\n",
        "        ids = torch.asarray([ids], device=\"cuda\", dtype=torch.int)\n",
        "        embedded = self.wrapped.model.token_embedding.wrapped(ids).squeeze(0)\n",
        "\n",
        "        return embedded\n",
        "\n",
        "model_hijack = StableDiffusionModelHijack()\n",
        "embedding_db = EmbeddingDatabase()\n",
        "model_hijack.hijack(model)\n",
        "model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "E0tJVYA4WM_u"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Animation Settings}}$\n",
        "def DeforumAnimArgs():\n",
        "\n",
        "    #@markdown ####**Animation:**\n",
        "    animation_mode = '3D' #@param ['None', '2D', '3D', 'Video Input', 'Interpolation'] {type:'string'}\n",
        "    max_frames = 6000 #@param {type:\"number\"}\n",
        "    border = 'wrap' #@param ['wrap', 'replicate'] {type:'string'}\n",
        "\n",
        "    #@markdown ####**Motion Parameters:**\n",
        "    angle = \"0:(0)\"#@param {type:\"string\"}\n",
        "    zoom = \"0:(1.02+0.02*sin(2*3.14*t/300))\"#@param {type:\"string\"}\n",
        "    translation_x = \"0:(3*sin(2*3.14*t/270))\"#@param {type:\"string\"}\n",
        "    translation_y = \"0:((3*sin(2*3.14*t/270)**20)-3)\"#@param {type:\"string\"}\n",
        "    translation_z = \"0: (2), 30: ((10*(sin(3.141*t/300)**900)+2) + (+0.1 *(sin(3.141*t/900)**50)+2))\"#@param {type:\"string\"}\n",
        "    rotation_3d_x = \"0:(0)\"#@param {type:\"string\"}\n",
        "    rotation_3d_y = \"0:(-0.2*sin(2*3.14*t/270))\"#@param {type:\"string\"}\n",
        "    rotation_3d_z = \"0: (0), 30: ((10*(sin(3.141*t/1500)**5000)+0.5) + (+0.1 *(sin(3.141*t/1500)**50)-0.5))\"#@param {type:\"string\"}\n",
        "    flip_2d_perspective = False #@param {type:\"boolean\"}\n",
        "    perspective_flip_theta = \"0:(0)\"#@param {type:\"string\"}\n",
        "    perspective_flip_phi = \"0:(t%15)\"#@param {type:\"string\"}\n",
        "    perspective_flip_gamma = \"0:(0)\"#@param {type:\"string\"}\n",
        "    perspective_flip_fv = \"0:(53)\"#@param {type:\"string\"}\n",
        "    noise_schedule = \"0:(.3), 30: (5*(cos(3.141*t/90)**5000)+0.03)\"#@param {type:\"string\"}\n",
        "    strength_schedule = \"0:(.60), 30: (-4*(cos(3.141*t/90)**1000)+0.60)\"#@param {type:\"string\"}\n",
        "    contrast_schedule = \"0: (1.0)\"#@param {type:\"string\"}\n",
        "    hybrid_video_comp_alpha_schedule = \"0:(1)\" #@param {type:\"string\"}\n",
        "    hybrid_video_comp_mask_blend_alpha_schedule = \"0:(0.5)\" #@param {type:\"string\"}\n",
        "    hybrid_video_comp_mask_contrast_schedule = \"0:(1)\" #@param {type:\"string\"}\n",
        "    hybrid_video_comp_mask_auto_contrast_cutoff_high_schedule =  \"0:(100)\" #@param {type:\"string\"}\n",
        "    hybrid_video_comp_mask_auto_contrast_cutoff_low_schedule =  \"0:(0)\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Unsharp mask (anti-blur) Parameters:**\n",
        "    kernel_schedule = \"0: (5)\"#@param {type:\"string\"}\n",
        "    sigma_schedule = \"0: (1.0)\"#@param {type:\"string\"}\n",
        "    amount_schedule = \"0: (0.2)\"#@param {type:\"string\"}\n",
        "    threshold_schedule = \"0: (0.0)\"#@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Coherence:**\n",
        "    color_coherence = 'Match Frame 0 RGB' #@param ['None', 'Match Frame 0 HSV', 'Match Frame 0 LAB', 'Match Frame 0 RGB', 'Video Input'] {type:'string'}\n",
        "    color_coherence_video_every_N_frames = 1 #@param {type:\"integer\"}\n",
        "    diffusion_cadence = '4' #@param ['1','2','3','4','5','6','7','8'] {type:'string'}\n",
        "\n",
        "    #@markdown ####**3D Depth Warping:**\n",
        "    use_depth_warping = True #@param {type:\"boolean\"}\n",
        "    midas_weight = 0.3#@param {type:\"number\"}\n",
        "    near_plane = 200\n",
        "    far_plane = 10000\n",
        "    fov = 40#@param {type:\"number\"}\n",
        "    padding_mode = 'border'#@param ['border', 'reflection', 'zeros'] {type:'string'}\n",
        "    sampling_mode = 'bicubic'#@param ['bicubic', 'bilinear', 'nearest'] {type:'string'}\n",
        "    save_depth_maps = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Video Input:**\n",
        "    video_init_path =''#@param {type:\"string\"}\n",
        "    extract_nth_frame = 1#@param {type:\"number\"}\n",
        "    overwrite_extracted_frames = True #@param {type:\"boolean\"}\n",
        "    use_mask_video = False #@param {type:\"boolean\"}\n",
        "    video_mask_path =''#@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Hybrid Video for 2D/3D Animation Mode:**\n",
        "    hybrid_video_generate_inputframes = False #@param {type:\"boolean\"}\n",
        "    hybrid_video_use_first_frame_as_init_image = False #@param {type:\"boolean\"}\n",
        "    hybrid_video_motion = \"None\" #@param ['None','Optical Flow','Perspective','Affine']\n",
        "    hybrid_video_flow_method = \"Farneback\" #@param ['Farneback','DenseRLOF','SF']\n",
        "    hybrid_video_composite = False #@param {type:\"boolean\"}\n",
        "    hybrid_video_comp_mask_type = \"None\" #@param ['None', 'Depth', 'Video Depth', 'Blend', 'Difference']\n",
        "    hybrid_video_comp_mask_inverse = False #@param {type:\"boolean\"}\n",
        "    hybrid_video_comp_mask_equalize = \"None\" #@param  ['None','Before','After','Both']\n",
        "    hybrid_video_comp_mask_auto_contrast = True #@param {type:\"boolean\"}\n",
        "    hybrid_video_comp_save_extra_frames = True #@param {type:\"boolean\"}\n",
        "    hybrid_video_use_video_as_mse_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Interpolation:**\n",
        "    interpolate_key_frames = False #@param {type:\"boolean\"}\n",
        "    interpolate_x_frames = 4 #@param {type:\"number\"}\n",
        "    \n",
        "    #@markdown ####**Resume Animation:**\n",
        "    resume_from_timestring = True #@param {type:\"boolean\"}\n",
        "    resume_timestring = \"20230126064112\" #@param {type:\"string\"}\n",
        "\n",
        "    return locals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i9fly1RIWM_u"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"a beautiful lake by Asher Brown Durand, trending on Artstation\", # the first prompt I want\n",
        "    \"a beautiful portrait of a woman by Artgerm, trending on Artstation\", # the second prompt I want\n",
        "    #\"this prompt I don't want it I commented it out\",\n",
        "    #\"a nousr robot, trending on Artstation\", # use \"nousr robot\" with the robot diffusion model (see model_checkpoint setting)\n",
        "    #\"touhou 1girl komeiji_koishi portrait, green hair\", # waifu diffusion prompts can use danbooru tag groups (see model_checkpoint)\n",
        "    #\"this prompt has weights if prompt weighting enabled:2 can also do negative:-2\", # (see prompt_weighting)\n",
        "]\n",
        "\n",
        "animation_prompts = {\n",
        "    0: \"a beautiful psychedelic dreamlike apple orchard, trending on Artstation\",\n",
        "    2000: \"a beautiful psychedelic dreamlike banana orchard, trending on Artstation\",\n",
        "    3000: \"a beautiful psychedelic dreamlike coconut orchard, trending on Artstation\",\n",
        "    4000: \"a beautiful psychedelic dreamlike durian orchard, trending on Artstation\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XVzhbmizWM_u"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Load Settings and Create Animation Frames}}$\n",
        "import time\n",
        "override_settings_with_file = False #@param {type:\"boolean\"}\n",
        "settings_file = \"custom\" #@param [\"custom\", \"512x512_aesthetic_0.json\",\"512x512_aesthetic_1.json\",\"512x512_colormatch_0.json\",\"512x512_colormatch_1.json\",\"512x512_colormatch_2.json\",\"512x512_colormatch_3.json\"]\n",
        "custom_settings_file = \"/content/drive/MyDrive/Settings.txt\"#@param {type:\"string\"}\n",
        "def DeforumArgs():\n",
        "    #@markdown **Image Settings**\n",
        "    W = 960 #@param\n",
        "    H = 512 #@param\n",
        "    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n",
        "    bit_depth_output = 8 #@param [8, 16, 32] {type:\"raw\"}\n",
        "\n",
        "    #@markdown **Sampling Settings**\n",
        "    seed = -1 #@param\n",
        "    sampler = 'dpm2_ancestral' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_a\", \"dpmpp_2m\"]\n",
        "    steps = 185 #@param\n",
        "    scale = 15 #@param\n",
        "    ddim_eta = 0.0 #@param\n",
        "    dynamic_threshold = None\n",
        "    static_threshold = None   \n",
        "\n",
        "    #@markdown **Save & Display Settings**\n",
        "    save_samples = True #@param {type:\"boolean\"}\n",
        "    save_settings = True #@param {type:\"boolean\"}\n",
        "    display_samples = True #@param {type:\"boolean\"}\n",
        "    save_sample_per_step = False #@param {type:\"boolean\"}\n",
        "    show_sample_per_step = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown **Prompt Settings**\n",
        "    prompt_weighting = False #@param {type:\"boolean\"}\n",
        "    normalize_prompt_weights = False #@param {type:\"boolean\"}\n",
        "    log_weighted_subprompts = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown **Batch Settings**\n",
        "    n_batch = 1 #@param\n",
        "    batch_name = \"dreamnix\" #@param {type:\"string\"}\n",
        "    filename_format = \"{timestring}_{index}_{prompt}.png\" #@param [\"{timestring}_{index}_{seed}.png\",\"{timestring}_{index}_{prompt}.png\"]\n",
        "    seed_behavior = \"iter\" #@param [\"iter\",\"fixed\",\"random\",\"ladder\",\"alternate\"]\n",
        "    seed_iter_N = 1 #@param {type:'integer'}\n",
        "    make_grid = False #@param {type:\"boolean\"}\n",
        "    grid_rows = 2 #@param \n",
        "    outdir = get_output_folder(root.output_path, batch_name)\n",
        "\n",
        "    #@markdown **Init Settings**\n",
        "    use_init = False #@param {type:\"boolean\"}\n",
        "    strength = 0.1 #@param {type:\"number\"}\n",
        "    strength_0_no_init = True # Set the strength to 0 automatically when no init image is used\n",
        "    init_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n",
        "    # Whiter areas of the mask are areas that change more\n",
        "    use_mask = False #@param {type:\"boolean\"}\n",
        "    use_alpha_as_mask = False # use the alpha channel of the init image as the mask\n",
        "    mask_file = \"https://www.filterforge.com/wiki/images/archive/b/b7/20080927223728%21Polygonal_gradient_thumb.jpg\" #@param {type:\"string\"}\n",
        "    invert_mask = False #@param {type:\"boolean\"}\n",
        "    # Adjust mask image, 1.0 is no adjustment. Should be positive numbers.\n",
        "    mask_brightness_adjust = 1.0  #@param {type:\"number\"}\n",
        "    mask_contrast_adjust = 1.0  #@param {type:\"number\"}\n",
        "    # Overlay the masked image at the end of the generation so it does not get degraded by encoding and decoding\n",
        "    overlay_mask = True  # {type:\"boolean\"}\n",
        "    # Blur edges of final overlay mask, if used. Minimum = 0 (no blur)\n",
        "    mask_overlay_blur = 5 # {type:\"number\"}\n",
        "\n",
        "    #@markdown **Exposure/Contrast Conditional Settings**\n",
        "    mean_scale = 0 #@param {type:\"number\"}\n",
        "    var_scale = 0 #@param {type:\"number\"}\n",
        "    exposure_scale = 0 #@param {type:\"number\"}\n",
        "    exposure_target = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **Color Match Conditional Settings**\n",
        "    colormatch_scale = 0 #@param {type:\"number\"}\n",
        "    colormatch_image = \"https://www.saasdesign.io/wp-content/uploads/2021/02/palette-3-min-980x588.png\" #@param {type:\"string\"}\n",
        "    colormatch_n_colors = 4 #@param {type:\"number\"}\n",
        "    ignore_sat_weight = 0 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **CLIP\\Aesthetics Conditional Settings**\n",
        "    clip_name = 'ViT-L/14' #@param ['ViT-L/14', 'ViT-L/14@336px', 'ViT-B/16', 'ViT-B/32']\n",
        "    clip_scale = 0 #@param {type:\"number\"}\n",
        "    aesthetics_scale = 0 #@param {type:\"number\"}\n",
        "    cutn = 0 #@param {type:\"number\"}\n",
        "    cut_pow = 0 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **Other Conditional Settings**\n",
        "    init_mse_scale = 0 #@param {type:\"number\"}\n",
        "    init_mse_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "    blue_scale = 0 #@param {type:\"number\"}\n",
        "    \n",
        "    #@markdown **Conditional Gradient Settings**\n",
        "    gradient_wrt = 'x' #@param [\"x\", \"x0_pred\"]\n",
        "    gradient_add_to = 'both' #@param [\"cond\", \"uncond\", \"both\"]\n",
        "    decode_method = 'linear' #@param [\"autoencoder\",\"linear\"]\n",
        "    grad_threshold_type = 'dynamic' #@param [\"dynamic\", \"static\", \"mean\", \"schedule\"]\n",
        "    clamp_grad_threshold = 0.2 #@param {type:\"number\"}\n",
        "    clamp_start = 0.2 #@param\n",
        "    clamp_stop = 0.01 #@param\n",
        "    grad_inject_timing = list(range(1,10)) #@param\n",
        "\n",
        "    #@markdown **Speed vs VRAM Settings**\n",
        "    cond_uncond_sync = True #@param {type:\"boolean\"}\n",
        "\n",
        "    n_samples = 1 # doesnt do anything\n",
        "    precision = 'autocast' \n",
        "    C = 4\n",
        "    f = 8\n",
        "\n",
        "    prompt = \"\"\n",
        "    timestring = \"\"\n",
        "    init_latent = None\n",
        "    init_sample = None\n",
        "    init_sample_raw = None\n",
        "    mask_sample = None\n",
        "    init_c = None\n",
        "    seed_internal = 0\n",
        "\n",
        "    return locals()\n",
        "\n",
        "args_dict = DeforumArgs()\n",
        "anim_args_dict = DeforumAnimArgs()\n",
        "\n",
        "if override_settings_with_file:\n",
        "    load_args(args_dict, anim_args_dict, settings_file, custom_settings_file, verbose=False)\n",
        "\n",
        "args = SimpleNamespace(**args_dict)\n",
        "anim_args = SimpleNamespace(**anim_args_dict)\n",
        "\n",
        "args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "args.strength = max(0.0, min(1.0, args.strength))\n",
        "\n",
        "# Load clip model if using clip guidance\n",
        "if (args.clip_scale > 0) or (args.aesthetics_scale > 0):\n",
        "    root.clip_model = clip.load(args.clip_name, jit=False)[0].eval().requires_grad_(False).to(root.device)\n",
        "    if (args.aesthetics_scale > 0):\n",
        "        root.aesthetics_model = load_aesthetics_model(args, root)\n",
        "\n",
        "if args.seed == -1:\n",
        "    args.seed = random.randint(0, 2**32 - 1)\n",
        "if not args.use_init:\n",
        "    args.init_image = None\n",
        "if args.sampler == 'plms' and (args.use_init or anim_args.animation_mode != 'None'):\n",
        "    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n",
        "    args.sampler = 'klms'\n",
        "if args.sampler != 'ddim':\n",
        "    args.ddim_eta = 0\n",
        "\n",
        "if anim_args.animation_mode == 'None':\n",
        "    anim_args.max_frames = 1\n",
        "elif anim_args.animation_mode == 'Video Input':\n",
        "    args.use_init = True\n",
        "\n",
        "# clean up unused memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# dispatch to appropriate renderer\n",
        "if anim_args.animation_mode == '2D' or anim_args.animation_mode == '3D':\n",
        "    render_animation(args, anim_args, animation_prompts, root)\n",
        "elif anim_args.animation_mode == 'Video Input':\n",
        "    render_input_video(args, anim_args, animation_prompts, root)\n",
        "elif anim_args.animation_mode == 'Interpolation':\n",
        "    render_interpolation(args, anim_args, animation_prompts, root)\n",
        "else:\n",
        "    render_image_batch(args, prompts, root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWNMwmr-JWuE"
      },
      "source": [
        " $\\color{blue} {\\large \\textsf{Create Video From Frames}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P0j4xHmQJWuE"
      },
      "outputs": [],
      "source": [
        "skip_video_for_run_all = True #@param {type: 'boolean'}\n",
        "fps = 12 #@param {type:\"number\"}\n",
        "#@markdown **Manual Settings**\n",
        "use_manual_settings = False #@param {type:\"boolean\"}\n",
        "image_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-01/StableFun/20230101212135_%05d.png\" #@param {type:\"string\"}\n",
        "mp4_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-01/StableFun/20230101212135.mp4\" #@param {type:\"string\"}\n",
        "render_steps = False  #@param {type: 'boolean'}\n",
        "path_name_modifier = \"x0_pred\" #@param [\"x0_pred\",\"x\"]\n",
        "make_gif = False\n",
        "bitdepth_extension = \"exr\" if args.bit_depth_output == 32 else \"png\"\n",
        "\n",
        "if skip_video_for_run_all == True:\n",
        "    print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "else:\n",
        "    import os\n",
        "    import subprocess\n",
        "    from base64 import b64encode\n",
        "\n",
        "    print(f\"{image_path} -> {mp4_path}\")\n",
        "\n",
        "    if use_manual_settings:\n",
        "        max_frames = \"200\" #@param {type:\"string\"}\n",
        "    else:\n",
        "        if render_steps: # render steps from a single image\n",
        "            fname = f\"{path_name_modifier}_%05d.png\"\n",
        "            all_step_dirs = [os.path.join(args.outdir, d) for d in os.listdir(args.outdir) if os.path.isdir(os.path.join(args.outdir,d))]\n",
        "            newest_dir = max(all_step_dirs, key=os.path.getmtime)\n",
        "            image_path = os.path.join(newest_dir, fname)\n",
        "            print(f\"Reading images from {image_path}\")\n",
        "            mp4_path = os.path.join(newest_dir, f\"{args.timestring}_{path_name_modifier}.mp4\")\n",
        "            max_frames = str(args.steps)\n",
        "        else: # render images for a video\n",
        "            image_path = os.path.join(args.outdir, f\"{args.timestring}_%05d.{bitdepth_extension}\")\n",
        "            mp4_path = os.path.join(args.outdir, f\"{args.timestring}.mp4\")\n",
        "            max_frames = str(anim_args.max_frames)\n",
        "\n",
        "    # make video\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec', bitdepth_extension,\n",
        "        '-r', str(fps),\n",
        "        '-start_number', str(0),\n",
        "        '-i', image_path,\n",
        "        '-frames:v', max_frames,\n",
        "        '-c:v', 'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        '-crf', '17',\n",
        "        '-preset', 'veryfast',\n",
        "        '-pattern_type', 'sequence',\n",
        "        mp4_path\n",
        "    ]\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        raise RuntimeError(stderr)\n",
        "\n",
        "    mp4 = open(mp4_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display.display(display.HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n",
        "    \n",
        "    if make_gif:\n",
        "         gif_path = os.path.splitext(mp4_path)[0]+'.gif'\n",
        "         cmd_gif = [\n",
        "             'ffmpeg',\n",
        "             '-y',\n",
        "             '-i', mp4_path,\n",
        "             '-r', str(fps),\n",
        "             gif_path\n",
        "         ]\n",
        "         process_gif = subprocess.Popen(cmd_gif, stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Z-jtRAVJWuG"
      },
      "outputs": [],
      "source": [
        "skip_disconnect_for_run_all = True #@param {type: 'boolean'}\n",
        "\n",
        "if skip_disconnect_for_run_all == True:\n",
        "    print('Skipping disconnect, uncheck skip_disconnect_for_run_all if you want to run it')\n",
        "else:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqZ6ZUNL8xS"
      },
      "source": [
        "# $ \\color{cyan} {\\large \\textsf{Model Merger}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "4nEedGxHeuXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb76ce6-6650-436c-beee-fbcec357fae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/FlonixDreamGen.ckpt...\n",
            "Loading /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/dreamlike-diffusion-1.0.ckpt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 2016/2016 [00:00<00:00, 2520.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Dreamlike-Gen1.0.ckpt...\n",
            "Merging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 2016/2016 [00:01<00:00, 1309.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baking in VAE from /content/drive/MyDrive/AI/models/vae-ft-mse-840000-ema-pruned.ckpt\n",
            "Saving to /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/FlonixGenDream.ckpt...\n",
            "Copying config:\n",
            "   from: /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Dreamlike-Gen1.0.yaml\n",
            "     to: /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/FlonixGenDream.yaml\n",
            "Checkpoint saved to /content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/FlonixGenDream.ckpt.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('FlonixGenDream', None)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#@title $ \\color{cyan} {\\large \\textsf{Merge Model!}}$\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from types import SimpleNamespace\n",
        "import safetensors.torch\n",
        "\n",
        "\n",
        "def Load_checkpoints_to_be_merged():\n",
        "  ckpt_dir = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\" #@param{type:'string'}\n",
        "  primary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Dreamlike-Gen1.0.ckpt\" #@param{type:'string'}\n",
        "  secondary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/FlonixDreamGen.ckpt\" #@param{type:'string'}\n",
        "  tertiary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/dreamlike-diffusion-1.0.ckpt\" #@param{type:'string'}\n",
        "  output_modelname = \"FlonixGenDream\" #@param{type:'string'}\n",
        "  config_source = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Dreamlike-Gen1.0.yaml\" #@param{type:'string'}\n",
        "  map_location = \"cuda\" #@param['cuda', 'cpu']\n",
        "  interp_method = \"Add difference\" #@param[\"No interpolation\", \"Weighted sum\", \"Add difference\"]\n",
        "  multiplier = 0.7 #@param{type:'slider', min:0.1, max:1, step:0.1}\n",
        "  save_as_half = False #@param{type:'boolean'}\n",
        "  custom_name = \"cuda\" #@param['cuda', 'cpu']\n",
        "  checkpoint_format = \"ckpt\" #@param['ckpt', 'safetensor']\n",
        "  bake_in_vae = \"/content/drive/MyDrive/AI/models/vae-ft-mse-840000-ema-pruned.ckpt\" #@param{type:'string'}\n",
        "  discard_weights = True #@param{type:'boolean'}\n",
        "  return locals()\n",
        "\n",
        "load_checkpoints_to_be_merged = Load_checkpoints_to_be_merged()\n",
        "load_checkpoints_to_be_merged = SimpleNamespace(**load_checkpoints_to_be_merged)\n",
        "\n",
        "chckpoint_dict_replacements = {\n",
        "    'cond_stage_model.transformer.embeddings.': 'cond_stage_model.transformer.text_model.embeddings.',\n",
        "    'cond_stage_model.transformer.encoder.': 'cond_stage_model.transformer.text_model.encoder.',\n",
        "    'cond_stage_model.transformer.final_layer_norm.': 'cond_stage_model.transformer.text_model.final_layer_norm.',\n",
        "}\n",
        "\n",
        "def _load_vae_dict(model, vae_dict_1):\n",
        "    model.first_stage_model.load_state_dict(vae_dict_1)\n",
        "    model.first_stage_model.to(\"cuda\")\n",
        "\n",
        "def load_vae_dict(filename, map_location):\n",
        "    vae_ckpt = read_state_dict(filename, map_location=map_location)\n",
        "    vae_dict_1 = {k: v for k, v in vae_ckpt.items() if k[0:4] != \"loss\" and k not in vae_ignore_keys}\n",
        "    return vae_dict_1\n",
        "\n",
        "\n",
        "def load_vae(model, vae_file=None, vae_source=\"from unknown source\"):\n",
        "    global vae_dict, loaded_vae_file\n",
        "    # save_settings = False\n",
        "\n",
        "\n",
        "    if vae_file:\n",
        "      _load_vae_dict(model, vae_file)\n",
        "    else:\n",
        "      assert os.path.isfile(vae_file), f\"VAE {vae_source} doesn't exist: {vae_file}\"\n",
        "      print(f\"Loading VAE weights {vae_source}: {vae_file}\")\n",
        "            \n",
        "\n",
        "    vae_dict_1 = load_vae_dict(vae_file, load_checkpoints_to_be_merged.map_location)\n",
        "    _load_vae_dict(model, vae_dict_1)\n",
        "\n",
        "    vae_file = vae_dict_1.copy()\n",
        "\n",
        "    loaded_vae_file = vae_file\n",
        "\n",
        "\n",
        "\n",
        "def find_checkpoint_config(info):\n",
        "    config = os.path.splitext(load_checkpoints_to_be_merged.primary_model_name) + \".yaml\"\n",
        "    if os.path.exists(config):\n",
        "        return config\n",
        "\n",
        "    return config\n",
        "\n",
        "def transform_checkpoint_dict_key(k):\n",
        "    for text, replacement in chckpoint_dict_replacements.items():\n",
        "        if k.startswith(text):\n",
        "            k = replacement + k[len(text):]\n",
        "\n",
        "    return k\n",
        "\n",
        "def get_state_dict_from_checkpoint(pl_sd):\n",
        "    pl_sd = pl_sd.pop(\"state_dict\", pl_sd)\n",
        "    pl_sd.pop(\"state_dict\", None)\n",
        "\n",
        "    sd = {}\n",
        "    for k, v in pl_sd.items():\n",
        "        new_key = transform_checkpoint_dict_key(k)\n",
        "\n",
        "        if new_key is not None:\n",
        "            sd[new_key] = v\n",
        "\n",
        "    pl_sd.clear()\n",
        "    pl_sd.update(sd)\n",
        "\n",
        "    return pl_sd\n",
        "\n",
        "def read_state_dict(checkpoint_file, print_global_state=False, map_location=None):\n",
        "    _, extension = os.path.splitext(checkpoint_file)\n",
        "    if extension.lower() == \".safetensors\":\n",
        "        device = load_checkpoints_to_be_merged.map_location\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\n",
        "    else:\n",
        "        pl_sd = torch.load(checkpoint_file, map_location=map_location)\n",
        "\n",
        "    if print_global_state and \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "\n",
        "    sd = get_state_dict_from_checkpoint(pl_sd)\n",
        "    return sd\n",
        "\n",
        "def create_config(ckpt_result, config_source, a, b, c):\n",
        "\n",
        "    cfg = load_checkpoints_to_be_merged.config_source\n",
        "\n",
        "    filename, _ = os.path.splitext(load_checkpoints_to_be_merged.output_modelname)\n",
        "    checkpoint_filename = os.path.dirname(load_checkpoints_to_be_merged.primary_model_name) + \"/\" + ckpt_result + \".yaml\"\n",
        "    print(\"Copying config:\")\n",
        "    print(\"   from:\", cfg)\n",
        "    print(\"     to:\", checkpoint_filename)\n",
        "    shutil.copyfile(cfg, checkpoint_filename)\n",
        "\n",
        "\n",
        "checkpoint_dict_skip_on_merge = [\"cond_stage_model.transformer.text_model.embeddings.position_ids\"]\n",
        "\n",
        "\n",
        "def to_half(tensor, enable):\n",
        "    if enable and tensor.dtype == torch.float:\n",
        "        return tensor.half()\n",
        "\n",
        "    return tensor\n",
        "\n",
        "vae_ignore_keys = {\"model_ema.decay\", \"model_ema.num_updates\"}\n",
        "vae_dict = {}\n",
        "def run_modelmerger(primary_model_name, secondary_model_name, tertiary_model_name, interp_method, multiplier, save_as_half, custom_name, checkpoint_format, config_source, bake_in_vae, discard_weights):\n",
        "\n",
        "    def fail(message):\n",
        "        textinfo = message\n",
        "        return message\n",
        "\n",
        "    def weighted_sum(theta0, theta1, alpha):\n",
        "        return ((1 - alpha) * theta0) + (alpha * theta1)\n",
        "\n",
        "    def get_difference(theta1, theta2):\n",
        "        return theta1 - theta2\n",
        "\n",
        "    def add_difference(theta0, theta1_2_diff, alpha):\n",
        "        return theta0 + (alpha * theta1_2_diff)\n",
        "\n",
        "    def filename_weighted_sum():\n",
        "        a = load_checkpoints_to_be_merged.primary_model_name\n",
        "        b = load_checkpoints_to_be_merged.secondary_model_name\n",
        "        Ma = round(1 - load_checkpoints_to_be_merged.multiplier, 2)\n",
        "        Mb = round(load_checkpoints_to_be_merged.multiplier, 2)\n",
        "\n",
        "        return f\"{Ma}({a}) + {Mb}({b})\"\n",
        "\n",
        "    def filename_add_difference():\n",
        "        a = load_checkpoints_to_be_merged.primary_model_name\n",
        "        b = load_checkpoints_to_be_merged.secondary_model_name\n",
        "        c = load_checkpoints_to_be_merged.tertiary_model_name\n",
        "        M = round(load_checkpoints_to_be_merged.multiplier, 2)\n",
        "\n",
        "        return f\"{a} + {M}({b} - {c})\"\n",
        "\n",
        "    def filename_nothing():\n",
        "        return load_checkpoints_to_be_merged.primary_model_name\n",
        "\n",
        "    theta_funcs = {\n",
        "        \"Weighted sum\": (filename_weighted_sum, None, weighted_sum),\n",
        "        \"Add difference\": (filename_add_difference, get_difference, add_difference),\n",
        "        \"No interpolation\": (filename_nothing, None, None),\n",
        "    }\n",
        "    filename_generator, theta_func1, theta_func2 = theta_funcs[load_checkpoints_to_be_merged.interp_method]\n",
        "    job_count = (1 if theta_func1 else 0) + (1 if theta_func2 else 0)\n",
        "\n",
        "    if not primary_model_name:\n",
        "        return fail(\"Failed: Merging requires a primary model.\")\n",
        "\n",
        "    primary_model_info = load_checkpoints_to_be_merged.primary_model_name\n",
        "\n",
        "    if theta_func2 and not secondary_model_name:\n",
        "        return fail(\"Failed: Merging requires a secondary model.\")\n",
        "\n",
        "    secondary_model_info = load_checkpoints_to_be_merged.secondary_model_name if theta_func2 else None\n",
        "\n",
        "    if theta_func1 and not tertiary_model_name:\n",
        "        return fail(f\"Failed: Interpolation method ({interp_method}) requires a tertiary model.\")\n",
        "\n",
        "    tertiary_model_info = load_checkpoints_to_be_merged.tertiary_model_name if theta_func1 else None\n",
        "\n",
        "    result_is_inpainting_model = False\n",
        "\n",
        "    if theta_func2:\n",
        "        textinfo = f\"Loading B\"\n",
        "        print(f\"Loading {secondary_model_info}...\")\n",
        "        theta_1 = read_state_dict(secondary_model_info, map_location='cpu')\n",
        "    else:\n",
        "        theta_1 = None\n",
        "\n",
        "    if theta_func1:\n",
        "        textinfo = f\"Loading C\"\n",
        "        print(f\"Loading {tertiary_model_info}...\")\n",
        "        theta_2 = read_state_dict(tertiary_model_info, map_location='cpu')\n",
        "\n",
        "        textinfo = 'Merging B and C'\n",
        "        sampling_steps = len(theta_1.keys())\n",
        "        for key in tqdm.tqdm(theta_1.keys()):\n",
        "            if key in checkpoint_dict_skip_on_merge:\n",
        "                continue\n",
        "\n",
        "            if 'model' in key:\n",
        "                if key in theta_2:\n",
        "                    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n",
        "                    theta_1[key] = theta_func1(theta_1[key], t2)\n",
        "                else:\n",
        "                    theta_1[key] = torch.zeros_like(theta_1[key])\n",
        "\n",
        "            sampling_steps += 1\n",
        "        del theta_2\n",
        "\n",
        "\n",
        "    textinfo = f\"Loading {primary_model_info}...\"\n",
        "    print(f\"Loading {primary_model_info}...\")\n",
        "    theta_0 = read_state_dict(primary_model_info, map_location='cpu')\n",
        "\n",
        "    print(\"Merging...\")\n",
        "    textinfo = 'Merging A and B'\n",
        "    sampling_steps = len(theta_0.keys())\n",
        "    for key in tqdm.tqdm(theta_0.keys()):\n",
        "        if theta_1 and 'model' in key and key in theta_1:\n",
        "\n",
        "            if key in checkpoint_dict_skip_on_merge:\n",
        "                continue\n",
        "\n",
        "            a = theta_0[key]\n",
        "            b = theta_1[key]\n",
        "\n",
        "            # this enables merging an inpainting model (A) with another one (B);\n",
        "            # where normal model would have 4 channels, for latenst space, inpainting model would\n",
        "            # have another 4 channels for unmasked picture's latent space, plus one channel for mask, for a total of 9\n",
        "            if a.shape != b.shape and a.shape[0:1] + a.shape[2:] == b.shape[0:1] + b.shape[2:]:\n",
        "                if a.shape[1] == 4 and b.shape[1] == 9:\n",
        "                    raise RuntimeError(\"When merging inpainting model with a normal one, A must be the inpainting model.\")\n",
        "\n",
        "                assert a.shape[1] == 9 and b.shape[1] == 4, f\"Bad dimensions for merged layer {key}: A={a.shape}, B={b.shape}\"\n",
        "\n",
        "                theta_0[key][:, 0:4, :, :] = theta_func2(a[:, 0:4, :, :], b, multiplier)\n",
        "                result_is_inpainting_model = True\n",
        "            else:\n",
        "                theta_0[key] = theta_func2(a, b, multiplier)\n",
        "\n",
        "            theta_0[key] = to_half(theta_0[key], save_as_half)\n",
        "\n",
        "        sampling_steps += 1\n",
        "\n",
        "    del theta_1\n",
        "\n",
        "    bake_in_vae_filename = load_checkpoints_to_be_merged.bake_in_vae\n",
        "    if bake_in_vae_filename is not None:\n",
        "        print(f\"Baking in VAE from {bake_in_vae_filename}\")\n",
        "        textinfo = 'Baking in VAE'\n",
        "        vae_dict = load_vae_dict(bake_in_vae_filename, map_location='cpu')\n",
        "\n",
        "        for key in vae_dict.keys():\n",
        "            theta_0_key = 'first_stage_model.' + key\n",
        "            if theta_0_key in theta_0:\n",
        "                theta_0[theta_0_key] = to_half(vae_dict[key], save_as_half)\n",
        "\n",
        "        del vae_dict\n",
        "\n",
        "    if save_as_half and not theta_func2:\n",
        "        for key in theta_0.keys():\n",
        "            theta_0[key] = to_half(theta_0[key], save_as_half)\n",
        "\n",
        "    # if discard_weights:\n",
        "    #     regex = re.compile(discard_weights)\n",
        "    #     for key in list(theta_0):\n",
        "    #         if re.search(regex, key):\n",
        "    #             theta_0.pop(key, None)\n",
        "\n",
        "    ckpt_dir = load_checkpoints_to_be_merged.ckpt_dir\n",
        "    filename = filename_generator() if custom_name == '' else custom_name\n",
        "    filename += \".inpainting\" if result_is_inpainting_model else \"\"\n",
        "    filename += \".\" + checkpoint_format\n",
        "\n",
        "    output_modelname = os.path.join(ckpt_dir, filename)\n",
        "\n",
        "    textinfo = \"Saving\"\n",
        "    print(f\"Saving to {output_modelname}...\")\n",
        "\n",
        "    _, extension = os.path.splitext(output_modelname)\n",
        "    if extension.lower() == \".safetensors\":\n",
        "        safetensors.torch.save_file(theta_0, output_modelname, metadata={\"format\": \"pt\"})\n",
        "    else:\n",
        "        torch.save(theta_0, output_modelname)\n",
        "\n",
        "\n",
        "    ckpt_cfg = create_config(load_checkpoints_to_be_merged.output_modelname,\n",
        "                  load_checkpoints_to_be_merged.config_source,\n",
        "                  primary_model_info,\n",
        "                  secondary_model_info,\n",
        "                  tertiary_model_info)\n",
        "\n",
        "    print(f\"Checkpoint saved to {output_modelname}.\")\n",
        "    textinfo = \"Checkpoint saved\"\n",
        "    \n",
        "\n",
        "    return load_checkpoints_to_be_merged.output_modelname, ckpt_cfg\n",
        "\n",
        "run_modelmerger(load_checkpoints_to_be_merged.primary_model_name,\n",
        "                load_checkpoints_to_be_merged.secondary_model_name, \n",
        "                load_checkpoints_to_be_merged.tertiary_model_name, \n",
        "                load_checkpoints_to_be_merged.interp_method, \n",
        "                load_checkpoints_to_be_merged.multiplier, \n",
        "                load_checkpoints_to_be_merged.save_as_half, \n",
        "                load_checkpoints_to_be_merged.output_modelname, \n",
        "                load_checkpoints_to_be_merged.checkpoint_format, \n",
        "                load_checkpoints_to_be_merged.config_source, \n",
        "                load_checkpoints_to_be_merged.bake_in_vae, \n",
        "                load_checkpoints_to_be_merged.discard_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{salmon} {\\large \\textsf{REAL-ESRGAN Video Upscaling!}}$\n"
      ],
      "metadata": {
        "id": "qGAjrH5REUBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WHYkZD6tuF4s"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{salmon} {\\large \\textsf{Mount Drive and Install Dependencies}}$\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "!pip install -r requirements.txt\n",
        "!pip install --user ffmpeg-python\n",
        "!pip install -U torch\n",
        "!pip install -U torchvision\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "BuNwcv9xuCDW"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{salmon} {\\large \\textsf { ð¹ RUN REAL-ESRGAN VIDEO UPSCALE PHASE (from video) ð¹}}$ \n",
        "model_name = 'realesr-general-x4v3' #@param ['realesr-general-x4v3', 'realesr-general-wdn-x4v3', 'RealESRGAN_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus'] {type:\"string\"}\n",
        "lowres_vid = '/content/drive/MyDrive/kong_rex.mov' #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input your lowres video path here to be upscaled}}$\n",
        "highres_vid_dir = '/content/drive/MyDrive/upscaled_storage/' #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input your high res video output path here}}$\n",
        "if not os.path.exists(highres_vid_dir):\n",
        "  os.makedirs(highres_vid_dir)\n",
        "outscale = \"4\" #@param ['1', '2', '3', '4', '5', '6']\n",
        "denoising_strength = 1 #@param{type:'slider', step:0.01, min:0.01, max:1}\n",
        "target_fps = 24 #@param{type:'number'}\n",
        "fp32 = 'fp32'\n",
        "gpu_id = 0 #@param ['0', '1', '2', '3', '4']\n",
        "\n",
        "%cd '/content/Real-ESRGAN/'\n",
        "!python3 /content/Real-ESRGAN/inference_realesrgan_video.py --model_name $model_name --outscale $outscale -dn $denoising_strength --fp32 --input $lowres_vid --fps $target_fps --output $highres_vid_dir\n",
        "\n",
        "%cd '/content/'\n",
        "print(\"...Upscaling Phase Has Completed, You May Proceed...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{orange} {\\large \\textsf{RIFE 4.6 Interpolation!}}$\n"
      ],
      "metadata": {
        "id": "Gjk7GkzMEfBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-q6DTPjC80p6"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf{Mount Drive and Install Dependencies}}$\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!git clone https://github.com/hzwer/Practical-RIFE.git\n",
        "%cd Practical-RIFE\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K9cFuhyW91kH"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf{Define Params and Run RIFE!}}$\n",
        "\n",
        "input_RIFE = '/content/drive/MyDrive/upscaled_storage/kong_rex_out.mov' # @param {type: 'string'}\n",
        "#@markdown THIS IS THE FILE NAME FOR YOUR COMPRESSED VID\n",
        "\n",
        "output_RIFE = '/content/drive/MyDrive/upscaled_storage/kong_rex_out_RIFE.mp4' # @param {type: 'string'}\n",
        "#@markdown INPUT YOUR OUTPUT NAME FOR THE INTERPOLATED VIDEO\n",
        "fps_RIFE = 60 # @param {type: 'number'}\n",
        "target_length_RIFE = 409 # @param {type: 'number'}\n",
        "target_scale_RIFE = 0.5 #@param ['0.25', '0.5', '1.0', '2.0', '4.0']\n",
        "_skip_RIFEsmoothing = False\n",
        "#@markdown THIS IS THE FILE NAME FOR YOUR COMPRESSED VID\n",
        "print(\"\\n ------------------------------------\\n\")\n",
        "print(\"\\n Beginning RIFE motion smoothing phase... \\n\")\n",
        "\n",
        "%cd /content/drive/MyDrive/Practical-RIFE/\n",
        "\n",
        "class Detection:\n",
        "  #def __init__(self):\n",
        "  #  pass\n",
        "  def detect_fps(input): #needs portable\n",
        "    import re\n",
        "    fps_ffprobe = !ffprobe -v error -select_streams v -of default=noprint_wrappers=1:nokey=1 -show_entries stream=avg_frame_rate $input\n",
        "    fps_unfinished = [str(i) for i in fps_ffprobe] # Converting integers into strings\n",
        "    fps_unfinishedTwo = str(\"\".join(fps_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', fps_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    fps = int(a_string)\n",
        "    #print(\"Detected FPS is\",fps)\n",
        "    return fps\n",
        "  def detect_duration(input):  #needs portable\n",
        "    import re\n",
        "    duration_ffprobe = !ffprobe -v error -select_streams v:0 -show_entries stream=duration -of default=noprint_wrappers=1:nokey=1 $input\n",
        "    duration_unfinished = [str(i) for i in duration_ffprobe] # Converting integers into strings\n",
        "    duration_unfinishedTwo = str(\"\".join(duration_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', duration_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    duration = float(int(a_string))\n",
        "    #print(\"Detected duration INTEGER (in seconds) is\",duration)\n",
        "    return duration\n",
        "  def exp_calc(measured_duration,target_length_RIFE): #needs portable\n",
        "    import numpy as np\n",
        "    a = measured_fps * measured_duration\n",
        "    b = fps_RIFE * target_length_RIFE\n",
        "    c = b / a\n",
        "    l = np.log(c) / np.log(2)\n",
        "    print(\"Un-rounded --exp is\",l)\n",
        "    x = round(l)\n",
        "    if x < 1:\n",
        "      x = 1\n",
        "    print(\"Rounding up to an --exp of \",x)\n",
        "    return x\n",
        "\n",
        "#----------------------------\n",
        "_import_mp4_file = False\n",
        "\n",
        "if _import_mp4_file:\n",
        "  measured_fps = Detection.detect_fps(input_RIFE)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",input_RIFE,\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(input_RIFE)\n",
        "else: \n",
        "  measured_fps = Detection.detect_fps(input_RIFE)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",{input_RIFE},\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(input_RIFE)\n",
        "\n",
        "print(\"\\n NOTICE: Detected duration INTEGER (in seconds) is \",measured_duration)\n",
        "\n",
        "if measured_duration < 1: #failsafe\n",
        "  print(\"\\n NOTICE: Your input appears to be less than one second... \\n\")\n",
        "  measured_duration = 1\n",
        "\n",
        "exp_value = Detection.exp_calc(measured_duration,target_length_RIFE)\n",
        "\n",
        "\n",
        "print(\"\\n NOTICE: Target duration currently rounds to the closest --exp RIFE can handle. \\n\")\n",
        "\n",
        "if (exp_value < 0.5):\n",
        "  _skip_RIFEsmoothing = False\n",
        "  print(\"\\n NOTICE: Your fps_RIFE doesn't necessitate RIFE motion smoothing. Skipping RIFE...\\n\")\n",
        "\n",
        "if _skip_RIFEsmoothing:\n",
        "  print(\"\\n NOTICE: Skipping RIFE motion smoothing...\\n\")\n",
        "else:\n",
        "  #---RUN RIFE------------------------------------\n",
        "  print(\"\\n NOTICE: Running RIFE... \\n\")\n",
        "  %cd /content/Practical-RIFE/\n",
        "  !python3 /content/Practical-RIFE/inference_video.py --fps=$fps_RIFE --exp=$exp_value  --video=$input_RIFE --scale=$target_scale_RIFE --output=$output_RIFE\n",
        "  #--exp=$exp_value\n",
        "  #---END--------------------------------------\n",
        "#!ffmpeg -y -i $input $visual_effects -c:v hevc_nvenc -rc vbr -cq $_constant_quality -qmin $_constant_quality -qmax $_constant_quality -b:v 0 $output_fullpathname\n",
        "#END OF MOTION SMOOTHING PHASE\n",
        "print(\"\\n End of RIFE interpolation phase.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wb8AOszeOzxY"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf {Analyze Lowres Vid Dimensions and FPS}}$ \n",
        "vcap = cv2.VideoCapture(output_RIFE)\n",
        "\n",
        "width = vcap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
        "height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
        "fps =  vcap.get(cv2.CAP_PROP_FPS)\n",
        "print(f\"Width is: {width}.\")\n",
        "print(f\"Height is: {height}.\")\n",
        "print(f\"FPS is: {fps}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IjlJkBoT8WCZ"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf{Disconnect Runtime}}$\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "qGAjrH5REUBm",
        "Gjk7GkzMEfBU"
      ],
      "authorship_tag": "ABX9TyPxuVIz50OM3cw1f/m6QRLC",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
